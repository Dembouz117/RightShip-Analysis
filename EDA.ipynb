{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "980e471f-41ca-41c0-a7bf-3c9fdcac9104",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              mmsi  vessel_id        vessel_name date_of_build  \\\n",
      "imo                                                              \n",
      "9223253  525119038     692017  PRIMA TANGGUH LVI    2002-04-01   \n",
      "9223253  525119038     692017  PRIMA TANGGUH LVI    2002-04-01   \n",
      "9223253  525119038     692017  PRIMA TANGGUH LVI    2002-04-01   \n",
      "9223253  525119038     692017  PRIMA TANGGUH LVI    2002-04-01   \n",
      "9223253  525119038     692017  PRIMA TANGGUH LVI    2002-04-01   \n",
      "...            ...        ...                ...           ...   \n",
      "9708693  374766000     714547          MSC CLARA    2015-11-01   \n",
      "9708693  374766000     714547          MSC CLARA    2015-11-01   \n",
      "9449106  636020361     647347        MENDELSSOHN    2012-04-01   \n",
      "9449106  636020361     647347        MENDELSSOHN    2012-04-01   \n",
      "9450313  636091832     647462  Northern Priority    2009-10-01   \n",
      "\n",
      "                             vessel_type                    group  \\\n",
      "imo                                                                 \n",
      "9223253        Crude/Oil Products Tanker  Crude & Products Tanker   \n",
      "9223253        Crude/Oil Products Tanker  Crude & Products Tanker   \n",
      "9223253        Crude/Oil Products Tanker  Crude & Products Tanker   \n",
      "9223253        Crude/Oil Products Tanker  Crude & Products Tanker   \n",
      "9223253        Crude/Oil Products Tanker  Crude & Products Tanker   \n",
      "...                                  ...                      ...   \n",
      "9708693  Container Ship (Fully Cellular)           Container Ship   \n",
      "9708693  Container Ship (Fully Cellular)           Container Ship   \n",
      "9449106  Container Ship (Fully Cellular)           Container Ship   \n",
      "9449106  Container Ship (Fully Cellular)           Container Ship   \n",
      "9450313  Container Ship (Fully Cellular)           Container Ship   \n",
      "\n",
      "                        timestamp   date_only         lon       lat  ...  \\\n",
      "imo                                                                  ...   \n",
      "9223253  2023-09-17T00:01:42.000Z  2023-09-17  103.826065  1.219275  ...   \n",
      "9223253  2023-09-17T00:01:44.000Z  2023-09-17  103.826065  1.219275  ...   \n",
      "9223253  2023-09-17T00:07:43.000Z  2023-09-17  103.825690  1.219517  ...   \n",
      "9223253  2023-09-17T00:07:45.000Z  2023-09-17  103.825690  1.219517  ...   \n",
      "9223253  2023-09-17T00:10:42.000Z  2023-09-17  103.825580  1.219637  ...   \n",
      "...                           ...         ...         ...       ...  ...   \n",
      "9708693  2023-08-21T21:59:17.000Z  2023-08-21  103.761665  1.271667  ...   \n",
      "9708693  2023-08-21T22:02:17.000Z  2023-08-21  103.761665  1.271667  ...   \n",
      "9449106  2023-08-21T21:58:47.000Z  2023-08-21  103.783330  1.255000  ...   \n",
      "9449106  2023-08-21T22:01:47.000Z  2023-08-21  103.783330  1.255000  ...   \n",
      "9450313  2023-08-21T22:01:40.000Z  2023-08-21  103.766670  1.268333  ...   \n",
      "\n",
      "         terminal       maneuvering_zone        p  vref  sfc_me sfc_ae sfc_ab  \\\n",
      "imo                                                                             \n",
      "9223253       NaN                    NaN   7860.0  14.5  185.00  230.0  300.0   \n",
      "9223253       NaN                    NaN   7860.0  14.5  185.00  230.0  300.0   \n",
      "9223253       NaN                    NaN   7860.0  14.5  185.00  230.0  300.0   \n",
      "9223253       NaN                    NaN   7860.0  14.5  185.00  230.0  300.0   \n",
      "9223253       NaN                    NaN   7860.0  14.5  185.00  230.0  300.0   \n",
      "...           ...                    ...      ...   ...     ...    ...    ...   \n",
      "9708693       NaN  PPT Maneouvering Zone  62500.0  19.0  175.00  220.0  300.0   \n",
      "9708693       NaN  PPT Maneouvering Zone  62500.0  19.0  175.00  220.0  300.0   \n",
      "9449106       NaN  PPT Maneouvering Zone  31990.0  23.0  174.83  220.0  300.0   \n",
      "9449106       NaN  PPT Maneouvering Zone  31990.0  23.0  174.83  220.0  300.0   \n",
      "9450313       NaN  PPT Maneouvering Zone  36560.0  22.5  172.91  195.0  300.0   \n",
      "\n",
      "            ael    abl                      geometry  \n",
      "imo                                                   \n",
      "9223253   507.0  365.0   POINT (103.826065 1.219275)  \n",
      "9223253   507.0  365.0   POINT (103.826065 1.219275)  \n",
      "9223253   507.0  365.0   POINT (103.82569 1.2195166)  \n",
      "9223253   507.0  365.0   POINT (103.82569 1.2195166)  \n",
      "9223253   507.0  365.0   POINT (103.82558 1.2196367)  \n",
      "...         ...    ...                           ...  \n",
      "9708693     NaN    NaN  POINT (103.761665 1.2716666)  \n",
      "9708693  4493.0  908.0  POINT (103.761665 1.2716666)  \n",
      "9449106     NaN    NaN       POINT (103.78333 1.255)  \n",
      "9449106  2503.0  436.0       POINT (103.78333 1.255)  \n",
      "9450313     NaN    NaN   POINT (103.76667 1.2683333)  \n",
      "\n",
      "[2822599 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Create a SparkSession - not needed if Dom on cloud\n",
    "spark = SparkSession.builder.appName(\"in_berth_example\").getOrCreate()\n",
    "\n",
    "# Changed from Dom, continue to read from table if u using Azure since it's faster than reading from disk like me\n",
    "vessel_pd = pd.read_csv(\"./vessel_movements/vessel_movements_PPT.csv\")\n",
    "\n",
    "\n",
    "def create_point(lon, lat):\n",
    "    return Point(lon, lat)\n",
    "\n",
    "#The respective Point datatype is stored in geometry, not to be confused later on with the geodataframe geometry from the polygon data\n",
    "df['geometry'] = df.apply(lambda row: create_point(row['lon'], row['lat']), axis=1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmsi</th>\n",
       "      <th>vessel_id</th>\n",
       "      <th>vessel_name</th>\n",
       "      <th>date_of_build</th>\n",
       "      <th>vessel_type</th>\n",
       "      <th>group</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date_only</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>...</th>\n",
       "      <th>terminal</th>\n",
       "      <th>maneuvering_zone</th>\n",
       "      <th>p</th>\n",
       "      <th>vref</th>\n",
       "      <th>sfc_me</th>\n",
       "      <th>sfc_ae</th>\n",
       "      <th>sfc_ab</th>\n",
       "      <th>ael</th>\n",
       "      <th>abl</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imo</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9223253</th>\n",
       "      <td>525119038</td>\n",
       "      <td>692017</td>\n",
       "      <td>PRIMA TANGGUH LVI</td>\n",
       "      <td>2002-04-01</td>\n",
       "      <td>Crude/Oil Products Tanker</td>\n",
       "      <td>Crude &amp; Products Tanker</td>\n",
       "      <td>2023-09-17T00:01:42.000Z</td>\n",
       "      <td>2023-09-17</td>\n",
       "      <td>103.826065</td>\n",
       "      <td>1.219275</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7860.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>185.00</td>\n",
       "      <td>230.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>507.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>POINT (103.82606 1.21928)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9223253</th>\n",
       "      <td>525119038</td>\n",
       "      <td>692017</td>\n",
       "      <td>PRIMA TANGGUH LVI</td>\n",
       "      <td>2002-04-01</td>\n",
       "      <td>Crude/Oil Products Tanker</td>\n",
       "      <td>Crude &amp; Products Tanker</td>\n",
       "      <td>2023-09-17T00:01:44.000Z</td>\n",
       "      <td>2023-09-17</td>\n",
       "      <td>103.826065</td>\n",
       "      <td>1.219275</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7860.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>185.00</td>\n",
       "      <td>230.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>507.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>POINT (103.82606 1.21928)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9223253</th>\n",
       "      <td>525119038</td>\n",
       "      <td>692017</td>\n",
       "      <td>PRIMA TANGGUH LVI</td>\n",
       "      <td>2002-04-01</td>\n",
       "      <td>Crude/Oil Products Tanker</td>\n",
       "      <td>Crude &amp; Products Tanker</td>\n",
       "      <td>2023-09-17T00:07:43.000Z</td>\n",
       "      <td>2023-09-17</td>\n",
       "      <td>103.825690</td>\n",
       "      <td>1.219517</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7860.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>185.00</td>\n",
       "      <td>230.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>507.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>POINT (103.82569 1.21952)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9223253</th>\n",
       "      <td>525119038</td>\n",
       "      <td>692017</td>\n",
       "      <td>PRIMA TANGGUH LVI</td>\n",
       "      <td>2002-04-01</td>\n",
       "      <td>Crude/Oil Products Tanker</td>\n",
       "      <td>Crude &amp; Products Tanker</td>\n",
       "      <td>2023-09-17T00:07:45.000Z</td>\n",
       "      <td>2023-09-17</td>\n",
       "      <td>103.825690</td>\n",
       "      <td>1.219517</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7860.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>185.00</td>\n",
       "      <td>230.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>507.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>POINT (103.82569 1.21952)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9223253</th>\n",
       "      <td>525119038</td>\n",
       "      <td>692017</td>\n",
       "      <td>PRIMA TANGGUH LVI</td>\n",
       "      <td>2002-04-01</td>\n",
       "      <td>Crude/Oil Products Tanker</td>\n",
       "      <td>Crude &amp; Products Tanker</td>\n",
       "      <td>2023-09-17T00:10:42.000Z</td>\n",
       "      <td>2023-09-17</td>\n",
       "      <td>103.825580</td>\n",
       "      <td>1.219637</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7860.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>185.00</td>\n",
       "      <td>230.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>507.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>POINT (103.82558 1.21964)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9708693</th>\n",
       "      <td>374766000</td>\n",
       "      <td>714547</td>\n",
       "      <td>MSC CLARA</td>\n",
       "      <td>2015-11-01</td>\n",
       "      <td>Container Ship (Fully Cellular)</td>\n",
       "      <td>Container Ship</td>\n",
       "      <td>2023-08-21T21:59:17.000Z</td>\n",
       "      <td>2023-08-21</td>\n",
       "      <td>103.761665</td>\n",
       "      <td>1.271667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PPT Maneouvering Zone</td>\n",
       "      <td>62500.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>175.00</td>\n",
       "      <td>220.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POINT (103.76166 1.27167)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9708693</th>\n",
       "      <td>374766000</td>\n",
       "      <td>714547</td>\n",
       "      <td>MSC CLARA</td>\n",
       "      <td>2015-11-01</td>\n",
       "      <td>Container Ship (Fully Cellular)</td>\n",
       "      <td>Container Ship</td>\n",
       "      <td>2023-08-21T22:02:17.000Z</td>\n",
       "      <td>2023-08-21</td>\n",
       "      <td>103.761665</td>\n",
       "      <td>1.271667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PPT Maneouvering Zone</td>\n",
       "      <td>62500.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>175.00</td>\n",
       "      <td>220.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>4493.0</td>\n",
       "      <td>908.0</td>\n",
       "      <td>POINT (103.76166 1.27167)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9449106</th>\n",
       "      <td>636020361</td>\n",
       "      <td>647347</td>\n",
       "      <td>MENDELSSOHN</td>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>Container Ship (Fully Cellular)</td>\n",
       "      <td>Container Ship</td>\n",
       "      <td>2023-08-21T21:58:47.000Z</td>\n",
       "      <td>2023-08-21</td>\n",
       "      <td>103.783330</td>\n",
       "      <td>1.255000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PPT Maneouvering Zone</td>\n",
       "      <td>31990.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>174.83</td>\n",
       "      <td>220.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POINT (103.78333 1.25500)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9449106</th>\n",
       "      <td>636020361</td>\n",
       "      <td>647347</td>\n",
       "      <td>MENDELSSOHN</td>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>Container Ship (Fully Cellular)</td>\n",
       "      <td>Container Ship</td>\n",
       "      <td>2023-08-21T22:01:47.000Z</td>\n",
       "      <td>2023-08-21</td>\n",
       "      <td>103.783330</td>\n",
       "      <td>1.255000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PPT Maneouvering Zone</td>\n",
       "      <td>31990.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>174.83</td>\n",
       "      <td>220.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>2503.0</td>\n",
       "      <td>436.0</td>\n",
       "      <td>POINT (103.78333 1.25500)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9450313</th>\n",
       "      <td>636091832</td>\n",
       "      <td>647462</td>\n",
       "      <td>Northern Priority</td>\n",
       "      <td>2009-10-01</td>\n",
       "      <td>Container Ship (Fully Cellular)</td>\n",
       "      <td>Container Ship</td>\n",
       "      <td>2023-08-21T22:01:40.000Z</td>\n",
       "      <td>2023-08-21</td>\n",
       "      <td>103.766670</td>\n",
       "      <td>1.268333</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PPT Maneouvering Zone</td>\n",
       "      <td>36560.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>172.91</td>\n",
       "      <td>195.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POINT (103.76667 1.26833)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2822599 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              mmsi  vessel_id        vessel_name date_of_build  \\\n",
       "imo                                                              \n",
       "9223253  525119038     692017  PRIMA TANGGUH LVI    2002-04-01   \n",
       "9223253  525119038     692017  PRIMA TANGGUH LVI    2002-04-01   \n",
       "9223253  525119038     692017  PRIMA TANGGUH LVI    2002-04-01   \n",
       "9223253  525119038     692017  PRIMA TANGGUH LVI    2002-04-01   \n",
       "9223253  525119038     692017  PRIMA TANGGUH LVI    2002-04-01   \n",
       "...            ...        ...                ...           ...   \n",
       "9708693  374766000     714547          MSC CLARA    2015-11-01   \n",
       "9708693  374766000     714547          MSC CLARA    2015-11-01   \n",
       "9449106  636020361     647347        MENDELSSOHN    2012-04-01   \n",
       "9449106  636020361     647347        MENDELSSOHN    2012-04-01   \n",
       "9450313  636091832     647462  Northern Priority    2009-10-01   \n",
       "\n",
       "                             vessel_type                    group  \\\n",
       "imo                                                                 \n",
       "9223253        Crude/Oil Products Tanker  Crude & Products Tanker   \n",
       "9223253        Crude/Oil Products Tanker  Crude & Products Tanker   \n",
       "9223253        Crude/Oil Products Tanker  Crude & Products Tanker   \n",
       "9223253        Crude/Oil Products Tanker  Crude & Products Tanker   \n",
       "9223253        Crude/Oil Products Tanker  Crude & Products Tanker   \n",
       "...                                  ...                      ...   \n",
       "9708693  Container Ship (Fully Cellular)           Container Ship   \n",
       "9708693  Container Ship (Fully Cellular)           Container Ship   \n",
       "9449106  Container Ship (Fully Cellular)           Container Ship   \n",
       "9449106  Container Ship (Fully Cellular)           Container Ship   \n",
       "9450313  Container Ship (Fully Cellular)           Container Ship   \n",
       "\n",
       "                        timestamp   date_only         lon       lat  ...  \\\n",
       "imo                                                                  ...   \n",
       "9223253  2023-09-17T00:01:42.000Z  2023-09-17  103.826065  1.219275  ...   \n",
       "9223253  2023-09-17T00:01:44.000Z  2023-09-17  103.826065  1.219275  ...   \n",
       "9223253  2023-09-17T00:07:43.000Z  2023-09-17  103.825690  1.219517  ...   \n",
       "9223253  2023-09-17T00:07:45.000Z  2023-09-17  103.825690  1.219517  ...   \n",
       "9223253  2023-09-17T00:10:42.000Z  2023-09-17  103.825580  1.219637  ...   \n",
       "...                           ...         ...         ...       ...  ...   \n",
       "9708693  2023-08-21T21:59:17.000Z  2023-08-21  103.761665  1.271667  ...   \n",
       "9708693  2023-08-21T22:02:17.000Z  2023-08-21  103.761665  1.271667  ...   \n",
       "9449106  2023-08-21T21:58:47.000Z  2023-08-21  103.783330  1.255000  ...   \n",
       "9449106  2023-08-21T22:01:47.000Z  2023-08-21  103.783330  1.255000  ...   \n",
       "9450313  2023-08-21T22:01:40.000Z  2023-08-21  103.766670  1.268333  ...   \n",
       "\n",
       "         terminal       maneuvering_zone        p  vref  sfc_me sfc_ae sfc_ab  \\\n",
       "imo                                                                             \n",
       "9223253       NaN                    NaN   7860.0  14.5  185.00  230.0  300.0   \n",
       "9223253       NaN                    NaN   7860.0  14.5  185.00  230.0  300.0   \n",
       "9223253       NaN                    NaN   7860.0  14.5  185.00  230.0  300.0   \n",
       "9223253       NaN                    NaN   7860.0  14.5  185.00  230.0  300.0   \n",
       "9223253       NaN                    NaN   7860.0  14.5  185.00  230.0  300.0   \n",
       "...           ...                    ...      ...   ...     ...    ...    ...   \n",
       "9708693       NaN  PPT Maneouvering Zone  62500.0  19.0  175.00  220.0  300.0   \n",
       "9708693       NaN  PPT Maneouvering Zone  62500.0  19.0  175.00  220.0  300.0   \n",
       "9449106       NaN  PPT Maneouvering Zone  31990.0  23.0  174.83  220.0  300.0   \n",
       "9449106       NaN  PPT Maneouvering Zone  31990.0  23.0  174.83  220.0  300.0   \n",
       "9450313       NaN  PPT Maneouvering Zone  36560.0  22.5  172.91  195.0  300.0   \n",
       "\n",
       "            ael    abl                   geometry  \n",
       "imo                                                \n",
       "9223253   507.0  365.0  POINT (103.82606 1.21928)  \n",
       "9223253   507.0  365.0  POINT (103.82606 1.21928)  \n",
       "9223253   507.0  365.0  POINT (103.82569 1.21952)  \n",
       "9223253   507.0  365.0  POINT (103.82569 1.21952)  \n",
       "9223253   507.0  365.0  POINT (103.82558 1.21964)  \n",
       "...         ...    ...                        ...  \n",
       "9708693     NaN    NaN  POINT (103.76166 1.27167)  \n",
       "9708693  4493.0  908.0  POINT (103.76166 1.27167)  \n",
       "9449106     NaN    NaN  POINT (103.78333 1.25500)  \n",
       "9449106  2503.0  436.0  POINT (103.78333 1.25500)  \n",
       "9450313     NaN    NaN  POINT (103.76667 1.26833)  \n",
       "\n",
       "[2822599 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from shapely import wkt\n",
    "\n",
    "#now, ima convert it into a geodataframe -> If run into any issues uncomment the below code\n",
    "# df[\"geometry\"] = df[\"geometry\"].apply(wkt.loads)\n",
    "gdf = gpd.GeoDataFrame(df, crs='epsg:4326')\n",
    "display(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do not ever mutate the path and the berth dataframe lads these are for the official polygon data unless u loading from Cloud\n",
    "shapefile_path = './PPT_shapefiles/PPT_berths.shp'\n",
    "berth_gdf = gpd.read_file(shapefile_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the inBerth\n",
    "def is_in_berth(point, berth_gdf):\n",
    "    return berth_gdf.geometry.contains(point).any()\n",
    "\n",
    "# Apply the function to create the 'inBerth' column -> slow asf, find a native method that's better cos this is a for loop -ish\n",
    "gdf['inBerth'] = gdf['geometry'].apply(lambda point: is_in_berth(point, berth_gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "imo\n",
       "9223253    False\n",
       "9223253    False\n",
       "9223253    False\n",
       "9223253    False\n",
       "9223253    False\n",
       "           ...  \n",
       "9708693    False\n",
       "9708693    False\n",
       "9449106    False\n",
       "9449106    False\n",
       "9450313    False\n",
       "Name: inBerth, Length: 2822599, dtype: bool"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(gdf[\"inBerth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              mmsi  vessel_id         vessel_name date_of_build  \\\n",
      "imo                                                               \n",
      "9648790  566592000     733125        AQUA-TERRA 7    2012-06-01   \n",
      "9648790  566592000     733125        AQUA-TERRA 7    2012-06-01   \n",
      "9648790  566592000     733125        AQUA-TERRA 7    2012-06-01   \n",
      "9648790  566592000     733125        AQUA-TERRA 7    2012-06-01   \n",
      "9648790  566592000     733125        AQUA-TERRA 7    2012-06-01   \n",
      "...            ...        ...                 ...           ...   \n",
      "9830197  563066530     829113         PSA POLARIS    2018-10-01   \n",
      "9345996  305073000     806731  NORTHERN DEXTERITY    2008-04-01   \n",
      "9345996  305073000     806731  NORTHERN DEXTERITY    2008-04-01   \n",
      "9148647  352495000     691213        SOUL OF LUCK    1997-10-01   \n",
      "9148647  352495000     691213        SOUL OF LUCK    1997-10-01   \n",
      "\n",
      "                             vessel_type                    group  \\\n",
      "imo                                                                 \n",
      "9648790                 Bunkering Tanker  Crude & Products Tanker   \n",
      "9648790                 Bunkering Tanker  Crude & Products Tanker   \n",
      "9648790                 Bunkering Tanker  Crude & Products Tanker   \n",
      "9648790                 Bunkering Tanker  Crude & Products Tanker   \n",
      "9648790                 Bunkering Tanker  Crude & Products Tanker   \n",
      "...                                  ...                      ...   \n",
      "9830197                              Tug                 Tug Boat   \n",
      "9345996  Container Ship (Fully Cellular)           Container Ship   \n",
      "9345996  Container Ship (Fully Cellular)           Container Ship   \n",
      "9148647  Container Ship (Fully Cellular)           Container Ship   \n",
      "9148647  Container Ship (Fully Cellular)           Container Ship   \n",
      "\n",
      "                        timestamp   date_only         lon       lat  ...  \\\n",
      "imo                                                                  ...   \n",
      "9648790  2023-09-17T14:45:40.000Z  2023-09-17  103.779560  1.257873  ...   \n",
      "9648790  2023-09-17T14:46:10.000Z  2023-09-17  103.779550  1.257883  ...   \n",
      "9648790  2023-09-17T14:50:09.000Z  2023-09-17  103.779170  1.257998  ...   \n",
      "9648790  2023-09-17T14:50:20.000Z  2023-09-17  103.779160  1.258010  ...   \n",
      "9648790  2023-09-17T14:55:39.000Z  2023-09-17  103.779190  1.258077  ...   \n",
      "...                           ...         ...         ...       ...  ...   \n",
      "9830197  2023-08-21T22:02:37.000Z  2023-08-21  103.778336  1.258333  ...   \n",
      "9345996  2023-08-21T22:00:00.000Z  2023-08-21  103.761665  1.273333  ...   \n",
      "9345996  2023-08-21T22:02:59.000Z  2023-08-21  103.761665  1.273333  ...   \n",
      "9148647  2023-08-21T23:59:27.000Z  2023-08-21  103.763690  1.288717  ...   \n",
      "9148647  2023-08-21T23:59:28.000Z  2023-08-21  103.763690  1.288717  ...   \n",
      "\n",
      "              maneuvering_zone        p  vref  sfc_me  sfc_ae sfc_ab    ael  \\\n",
      "imo                                                                           \n",
      "9648790  PPT Maneouvering Zone   3236.0  11.8  195.00   230.0  300.0  647.0   \n",
      "9648790  PPT Maneouvering Zone   3236.0  11.8  195.00   230.0  300.0  647.0   \n",
      "9648790  PPT Maneouvering Zone   3236.0  11.8  195.00   230.0  300.0  647.0   \n",
      "9648790  PPT Maneouvering Zone   3236.0  11.8  195.00   230.0  300.0  647.0   \n",
      "9648790  PPT Maneouvering Zone   3236.0  11.8  195.00   230.0  300.0  647.0   \n",
      "...                        ...      ...   ...     ...     ...    ...    ...   \n",
      "9830197  PPT Maneouvering Zone   3060.0  12.0  195.00   230.0  300.0    NaN   \n",
      "9345996  PPT Maneouvering Zone  31920.0  22.2  173.25   195.0  300.0    NaN   \n",
      "9345996  PPT Maneouvering Zone  31920.0  22.2  173.25   195.0  300.0  900.0   \n",
      "9148647  PPT Maneouvering Zone  11954.0  19.0  195.00   230.0  300.0    NaN   \n",
      "9148647  PPT Maneouvering Zone  11954.0  19.0  195.00   230.0  300.0  818.0   \n",
      "\n",
      "           abl                   geometry inBerth  \n",
      "imo                                                \n",
      "9648790  675.0  POINT (103.77956 1.25787)    True  \n",
      "9648790  675.0  POINT (103.77955 1.25788)    True  \n",
      "9648790  675.0  POINT (103.77917 1.25800)    True  \n",
      "9648790  675.0  POINT (103.77916 1.25801)    True  \n",
      "9648790  675.0  POINT (103.77919 1.25808)    True  \n",
      "...        ...                        ...     ...  \n",
      "9830197    NaN  POINT (103.77834 1.25833)    True  \n",
      "9345996    NaN  POINT (103.76166 1.27333)    True  \n",
      "9345996  423.0  POINT (103.76166 1.27333)    True  \n",
      "9148647    NaN  POINT (103.76369 1.28872)    True  \n",
      "9148647  368.0  POINT (103.76369 1.28872)    True  \n",
      "\n",
      "[1177644 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "#test if legit -> legit cos got true\n",
    "print(gdf[gdf[\"inBerth\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#I converting into a pyspark dataframe again to prevent confusion and enable further processing. uty use which, but geodataframe is similar to pandas\n",
    "vessel_df = spark.createDataFrame(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The point is outside the shape.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/27 09:43:43 ERROR Executor: Exception in task 0.0 in stage 21.0 (TID 84)\n",
      "net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:199)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$2(BatchEvalPythonExec.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "24/01/27 09:43:43 WARN TaskSetManager: Lost task 0.0 in stage 21.0 (TID 84) (smus-mbp-2 executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:199)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$2(BatchEvalPythonExec.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "\n",
      "24/01/27 09:43:43 ERROR TaskSetManager: Task 0 in stage 21.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o225.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 84) (smus-mbp-2 executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:199)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$2(BatchEvalPythonExec.scala:67)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:199)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$2(BatchEvalPythonExec.scala:67)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m is_within_geometry_udf \u001b[38;5;241m=\u001b[39m udf(is_within_geometry, BooleanType())\n\u001b[1;32m     29\u001b[0m vessel_df \u001b[38;5;241m=\u001b[39m vessel_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minBerth\u001b[39m\u001b[38;5;124m\"\u001b[39m, is_within_geometry_udf(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m\"\u001b[39m), col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m---> 30\u001b[0m \u001b[43mvessel_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o225.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 84) (smus-mbp-2 executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:199)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$2(BatchEvalPythonExec.scala:67)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:199)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$2(BatchEvalPythonExec.scala:67)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import Point\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "import geopandas as gpd\n",
    "\n",
    "shapefile_path = './PPT_shapefiles/PPT_berths.shp'\n",
    "gdfile = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Create a UDF to check if the point is within the geometry\n",
    "def is_within_geometry(lon, lat):\n",
    "    point = Point(lon, lat)\n",
    "    return gdfile.geometry.contains(point).any()\n",
    "\n",
    "# Register the UDF\n",
    "is_within_geometry_udf = udf(is_within_geometry, BooleanType())\n",
    "vessel_df = vessel_df.withColumn(\"inBerth\", is_within_geometry_udf(col(\"lon\"), col(\"lat\")))\n",
    "vessel_df.limit(8).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1115b041-87f8-4c37-90c6-e56c5ac24dcb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0j/2ms5f4dx5rlfqts07c12w3t40000gp/T/ipykernel_77977/2474852731.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m condition_transit = (\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'speed'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nav_stat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/sql/functions.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mColumn\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mColumn\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_invoke_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/sql/functions.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \"\"\"\n\u001b[1;32m     92\u001b[0m     \u001b[0mInvokes\u001b[0m \u001b[0mJVM\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0midentified\u001b[0m \u001b[0mby\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mand\u001b[0m \u001b[0mwraps\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \"\"\"\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0mjf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_jvm_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "condition_transit = (\n",
    "    ((col('speed') > 1) | col('nav_stat').isin([0, 3, 4, 8, 12])) & \n",
    "    (col('maneuvering_zone') == 'null')\n",
    ")\n",
    "\n",
    "condition_anchorage = (\n",
    "    (col('speed') < 1) & \n",
    "    (col('anchorage') != 'null')\n",
    ")\n",
    "\n",
    "condition_maneuvering = (\n",
    "    ((col('speed') > 1) | col('nav_stat').isin([0, 3, 4, 8, 12])) & \n",
    "    (col('maneuvering_zone') != 'null')\n",
    ")\n",
    "\n",
    "condition_alongside_hotel = (\n",
    "    (col('speed') < 1) & \n",
    "    (col('berth') != 'null')\n",
    ")\n",
    "\n",
    "step_1 = prep.withColumn('operating_mode',\n",
    "                              when(condition_transit, 'Transit')\n",
    "                               .when(condition_anchorage, 'Anchorage')\n",
    "                               .when(condition_maneuvering, 'Maneuvering')\n",
    "                               .when(condition_alongside_hotel, 'Alongside/Hotel')\n",
    "                               .otherwise('Unknown'))\n",
    "\n",
    "display(step_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f94c7fe-89d9-420c-acac-f41ccc6556c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
       "\u001b[0;32m<command-775644083908589>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n",
       "\u001b[1;32m      1\u001b[0m \u001b[0mstep_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"imo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstep_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_diff\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfiltered_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_diff\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;31mTypeError\u001b[0m: 'GroupedData' object is not subscriptable"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n\u001b[0;32m<command-775644083908589>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstep_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"imo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstep_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_diff\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfiltered_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_diff\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mTypeError\u001b[0m: 'GroupedData' object is not subscriptable",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: 'GroupedData' object is not subscriptable",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "step_1.orderBy([\"imo\", \"timestamp\"], inplace=True)\n",
    "step_1[\"time_diff\"] = step_1.groupby('imo')['timestamp'].diff().dt().total_seconds()\n",
    "filtered_step = step_1[step_1[\"time_diff\"] <= 10000]\n",
    "\n",
    "print(filtered_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d051a564-72ed-494f-addf-1866eec8a319",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "\u001b[0;32m<command-775644083908604>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n",
       "\u001b[1;32m     10\u001b[0m \u001b[0;31m# Calculate time difference using lag function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m     11\u001b[0m \u001b[0mwindow_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"imo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m---> 12\u001b[0;31m \u001b[0mstep_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time_diff\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"long\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m     14\u001b[0m \u001b[0;31m# Filter rows where time difference is less than or equal to 10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n",
       "\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n",
       "\u001b[1;32m   3325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   3326\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m-> 3327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m   3328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   3329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n",
       "\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    204\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '(CAST(timestamp AS BIGINT) - lag(timestamp, 1, NULL) OVER (PARTITION BY hive_metastore.default.vessel_movements_ppt.imo ORDER BY timestamp ASC NULLS FIRST ROWS BETWEEN -1 FOLLOWING AND -1 FOLLOWING))' due to data type mismatch: argument 1 requires (timestamp or timestamp without time zone) type, however, 'CAST(timestamp AS BIGINT)' is of bigint type.;\n",
       "'Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#9904, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields]\n",
       "+- Sort [imo#7044L ASC NULLS FIRST, timestamp#9904 ASC NULLS FIRST], true\n",
       "   +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, cast(timestamp#7051 as timestamp) AS timestamp#9904, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n",
       "      +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n",
       "         +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n",
       "            +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n",
       "               +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n",
       "                  +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n",
       "                     +- Filter NOT (((anchorage#7066 = null) AND (maneuvering_zone#7068 = null)) OR (NOT (anchorage#7066 = null) AND NOT (maneuvering_zone#7068 = null)))\n",
       "                        +- SubqueryAlias hive_metastore.default.vessel_movements_ppt\n",
       "                           +- Relation hive_metastore.default.vessel_movements_ppt[imo#7044L,mmsi#7045L,vessel_id#7046L,vessel_name#7047,date_of_build#7048,vessel_type#7049,group#7050,timestamp#7051,date_only#7052,lon#7053,lat#7054,nav_stat#7055L,speed#7056,course#7057,heading#7058,fuel_category#7059L,main_engine_fuel_type#7060,aux_engine_fuel_type#7061,engine_type#7062,berth#7063,port_name#7064,coverage#7065,anchorage#7066,terminal#7067,... 8 more fields] parquet\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n\u001b[0;32m<command-775644083908604>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Calculate time difference using lag function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mwindow_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"imo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mstep_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time_diff\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"long\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Filter rows where time difference is less than or equal to 10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   3325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3326\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '(CAST(timestamp AS BIGINT) - lag(timestamp, 1, NULL) OVER (PARTITION BY hive_metastore.default.vessel_movements_ppt.imo ORDER BY timestamp ASC NULLS FIRST ROWS BETWEEN -1 FOLLOWING AND -1 FOLLOWING))' due to data type mismatch: argument 1 requires (timestamp or timestamp without time zone) type, however, 'CAST(timestamp AS BIGINT)' is of bigint type.;\n'Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#9904, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields]\n+- Sort [imo#7044L ASC NULLS FIRST, timestamp#9904 ASC NULLS FIRST], true\n   +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, cast(timestamp#7051 as timestamp) AS timestamp#9904, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n      +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n         +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n            +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n               +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n                  +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n                     +- Filter NOT (((anchorage#7066 = null) AND (maneuvering_zone#7068 = null)) OR (NOT (anchorage#7066 = null) AND NOT (maneuvering_zone#7068 = null)))\n                        +- SubqueryAlias hive_metastore.default.vessel_movements_ppt\n                           +- Relation hive_metastore.default.vessel_movements_ppt[imo#7044L,mmsi#7045L,vessel_id#7046L,vessel_name#7047,date_of_build#7048,vessel_type#7049,group#7050,timestamp#7051,date_only#7052,lon#7053,lat#7054,nav_stat#7055L,speed#7056,course#7057,heading#7058,fuel_category#7059L,main_engine_fuel_type#7060,aux_engine_fuel_type#7061,engine_type#7062,berth#7063,port_name#7064,coverage#7065,anchorage#7066,terminal#7067,... 8 more fields] parquet\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: cannot resolve '(CAST(timestamp AS BIGINT) - lag(timestamp, 1, NULL) OVER (PARTITION BY hive_metastore.default.vessel_movements_ppt.imo ORDER BY timestamp ASC NULLS FIRST ROWS BETWEEN -1 FOLLOWING AND -1 FOLLOWING))' due to data type mismatch: argument 1 requires (timestamp or timestamp without time zone) type, however, 'CAST(timestamp AS BIGINT)' is of bigint type.;\n'Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#9904, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields]\n+- Sort [imo#7044L ASC NULLS FIRST, timestamp#9904 ASC NULLS FIRST], true\n   +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, cast(timestamp#7051 as timestamp) AS timestamp#9904, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n      +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n         +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n            +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n               +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n                  +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n                     +- Filter NOT (((anchorage#7066 = null) AND (maneuvering_zone#7068 = null)) OR (NOT (anchorage#7066 = null) AND NOT (maneuvering_zone#7068 = null)))\n                        +- SubqueryAlias hive_metastore.default.vessel_movements_ppt\n                           +- Relation hive_metastore.default.vessel_movements_ppt[imo#7044L,mmsi#7045L,vessel_id#7046L,vessel_name#7047,date_of_build#7048,vessel_type#7049,group#7050,timestamp#7051,date_only#7052,lon#7053,lat#7054,nav_stat#7055L,speed#7056,course#7057,heading#7058,fuel_category#7059L,main_engine_fuel_type#7060,aux_engine_fuel_type#7061,engine_type#7062,berth#7063,port_name#7064,coverage#7065,anchorage#7066,terminal#7067,... 8 more fields] parquet\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import functions as f\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Convert timestamp column to timestamp type\n",
    "window_spec = Window.partitionBy('imo').orderBy('timestamp')\n",
    "step_1 = step_1.withColumn(\"timestamp\", F.col(\"timestamp\"))cast(\"long\")\n",
    "\n",
    "# Order dataframe by imo and timestamp columns\n",
    "step_1 = step_1.orderBy(\"imo\", \"timestamp\")\n",
    "\n",
    "# Calculate time difference using lag function\n",
    "window_spec = Window.partitionBy(\"imo\").orderBy(\"timestamp\")\n",
    "step_1 = step_1.withColumn(\"time_diff\", col(\"timestamp\").cast(\"long\") - lag(\"timestamp\").over(window_spec))\n",
    "\n",
    "# Filter rows where time difference is less than or equal to 10000\n",
    "filtered_step = step_1.filter(col(\"time_diff\") <= 10000)\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "filtered_step.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff27054b-c1d7-4d0b-898e-fc8eaeb61544",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "\u001b[0;32m<command-775644083908609>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n",
       "\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      4\u001b[0m \u001b[0mwindow_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstep_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time_diff'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'long'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      7\u001b[0m \u001b[0;31m# Filter away entries where the consecutive time is more than 10,000 seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n",
       "\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n",
       "\u001b[1;32m   3325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   3326\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m-> 3327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m   3328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   3329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n",
       "\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    204\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '(CAST(timestamp AS BIGINT) - lag(timestamp, 1, NULL) OVER (PARTITION BY hive_metastore.default.vessel_movements_ppt.imo ORDER BY timestamp ASC NULLS FIRST ROWS BETWEEN -1 FOLLOWING AND -1 FOLLOWING))' due to data type mismatch: argument 1 requires (timestamp or timestamp without time zone) type, however, 'CAST(timestamp AS BIGINT)' is of bigint type.;\n",
       "'Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#9904, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields]\n",
       "+- Sort [imo#7044L ASC NULLS FIRST, timestamp#9904 ASC NULLS FIRST], true\n",
       "   +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, cast(timestamp#7051 as timestamp) AS timestamp#9904, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n",
       "      +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n",
       "         +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n",
       "            +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n",
       "               +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n",
       "                  +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n",
       "                     +- Filter NOT (((anchorage#7066 = null) AND (maneuvering_zone#7068 = null)) OR (NOT (anchorage#7066 = null) AND NOT (maneuvering_zone#7068 = null)))\n",
       "                        +- SubqueryAlias hive_metastore.default.vessel_movements_ppt\n",
       "                           +- Relation hive_metastore.default.vessel_movements_ppt[imo#7044L,mmsi#7045L,vessel_id#7046L,vessel_name#7047,date_of_build#7048,vessel_type#7049,group#7050,timestamp#7051,date_only#7052,lon#7053,lat#7054,nav_stat#7055L,speed#7056,course#7057,heading#7058,fuel_category#7059L,main_engine_fuel_type#7060,aux_engine_fuel_type#7061,engine_type#7062,berth#7063,port_name#7064,coverage#7065,anchorage#7066,terminal#7067,... 8 more fields] parquet\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n\u001b[0;32m<command-775644083908609>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwindow_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstep_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time_diff'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'long'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Filter away entries where the consecutive time is more than 10,000 seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   3325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3326\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '(CAST(timestamp AS BIGINT) - lag(timestamp, 1, NULL) OVER (PARTITION BY hive_metastore.default.vessel_movements_ppt.imo ORDER BY timestamp ASC NULLS FIRST ROWS BETWEEN -1 FOLLOWING AND -1 FOLLOWING))' due to data type mismatch: argument 1 requires (timestamp or timestamp without time zone) type, however, 'CAST(timestamp AS BIGINT)' is of bigint type.;\n'Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#9904, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields]\n+- Sort [imo#7044L ASC NULLS FIRST, timestamp#9904 ASC NULLS FIRST], true\n   +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, cast(timestamp#7051 as timestamp) AS timestamp#9904, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n      +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n         +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n            +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n               +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n                  +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n                     +- Filter NOT (((anchorage#7066 = null) AND (maneuvering_zone#7068 = null)) OR (NOT (anchorage#7066 = null) AND NOT (maneuvering_zone#7068 = null)))\n                        +- SubqueryAlias hive_metastore.default.vessel_movements_ppt\n                           +- Relation hive_metastore.default.vessel_movements_ppt[imo#7044L,mmsi#7045L,vessel_id#7046L,vessel_name#7047,date_of_build#7048,vessel_type#7049,group#7050,timestamp#7051,date_only#7052,lon#7053,lat#7054,nav_stat#7055L,speed#7056,course#7057,heading#7058,fuel_category#7059L,main_engine_fuel_type#7060,aux_engine_fuel_type#7061,engine_type#7062,berth#7063,port_name#7064,coverage#7065,anchorage#7066,terminal#7067,... 8 more fields] parquet\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: cannot resolve '(CAST(timestamp AS BIGINT) - lag(timestamp, 1, NULL) OVER (PARTITION BY hive_metastore.default.vessel_movements_ppt.imo ORDER BY timestamp ASC NULLS FIRST ROWS BETWEEN -1 FOLLOWING AND -1 FOLLOWING))' due to data type mismatch: argument 1 requires (timestamp or timestamp without time zone) type, however, 'CAST(timestamp AS BIGINT)' is of bigint type.;\n'Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#9904, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields]\n+- Sort [imo#7044L ASC NULLS FIRST, timestamp#9904 ASC NULLS FIRST], true\n   +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, cast(timestamp#7051 as timestamp) AS timestamp#9904, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n      +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n         +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n            +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n               +- Sort [imo#7044L ASC NULLS FIRST, timestamp#7051 ASC NULLS FIRST], true\n                  +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n                     +- Filter NOT (((anchorage#7066 = null) AND (maneuvering_zone#7068 = null)) OR (NOT (anchorage#7066 = null) AND NOT (maneuvering_zone#7068 = null)))\n                        +- SubqueryAlias hive_metastore.default.vessel_movements_ppt\n                           +- Relation hive_metastore.default.vessel_movements_ppt[imo#7044L,mmsi#7045L,vessel_id#7046L,vessel_name#7047,date_of_build#7048,vessel_type#7049,group#7050,timestamp#7051,date_only#7052,lon#7053,lat#7054,nav_stat#7055L,speed#7056,course#7057,heading#7058,fuel_category#7059L,main_engine_fuel_type#7060,aux_engine_fuel_type#7061,engine_type#7062,berth#7063,port_name#7064,coverage#7065,anchorage#7066,terminal#7067,... 8 more fields] parquet\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "step_1 = step_1.withColumn('datetime', col())\n",
    "window_spec = Window().partitionBy('imo').orderBy('timestamp')\n",
    "step_1 = step_1.withColumn('time_diff', F.col('timestamp').cast('long') - F.lag('timestamp').over(window_spec))\n",
    "\n",
    "# Filter away entries where the consecutive time is more than 10,000 seconds\n",
    "filtered_df = step_1.filter((F.col('time_diff').isNull()) | (F.col('time_diff') <= 10000))\n",
    "\n",
    "# Drop the temporary 'time_diff' column if you don't need it anymore\n",
    "filtered_df = filtered_df.drop('time_diff')\n",
    "\n",
    "# Show or use the filtered DataFrame as needed\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fc062df-1e51-4cb8-8b77-08e511516154",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "\u001b[0;32m<command-775644083908614>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n",
       "\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      8\u001b[0m \u001b[0;31m# Filter away entries where the consecutive time is more than 10,000 seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfiltered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time_diff'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time_diff'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'INTERVAL 10000 SECONDS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n",
       "\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n",
       "\u001b[1;32m   2163\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   2164\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m-> 2165\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m   2166\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   2167\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"condition should be string or Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n",
       "\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    204\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve \"(time_diff <= INTERVAL '10000' SECOND)\" due to data type mismatch: the left and right operands of the binary operator have incompatible types (\"BIGINT\" and \"INTERVAL SECOND\").;\n",
       "'Filter ((isnull(time_diff#11777L) = false) OR (time_diff#11777L <= INTERVAL '10000' SECOND))\n",
       "+- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n",
       "   +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n",
       "      +- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n",
       "         +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n",
       "            +- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n",
       "               +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n",
       "                  +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n",
       "                     +- Filter NOT (NOT (anchorage#7066 = null) AND NOT (maneuvering_zone#7068 = null))\n",
       "                        +- SubqueryAlias hive_metastore.default.vessel_movements_ppt\n",
       "                           +- Relation hive_metastore.default.vessel_movements_ppt[imo#7044L,mmsi#7045L,vessel_id#7046L,vessel_name#7047,date_of_build#7048,vessel_type#7049,group#7050,timestamp#7051,date_only#7052,lon#7053,lat#7054,nav_stat#7055L,speed#7056,course#7057,heading#7058,fuel_category#7059L,main_engine_fuel_type#7060,aux_engine_fuel_type#7061,engine_type#7062,berth#7063,port_name#7064,coverage#7065,anchorage#7066,terminal#7067,... 8 more fields] parquet\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n\u001b[0;32m<command-775644083908614>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Filter away entries where the consecutive time is more than 10,000 seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfiltered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time_diff'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time_diff'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'INTERVAL 10000 SECONDS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   2163\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2164\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2165\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2166\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2167\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"condition should be string or Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve \"(time_diff <= INTERVAL '10000' SECOND)\" due to data type mismatch: the left and right operands of the binary operator have incompatible types (\"BIGINT\" and \"INTERVAL SECOND\").;\n'Filter ((isnull(time_diff#11777L) = false) OR (time_diff#11777L <= INTERVAL '10000' SECOND))\n+- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n   +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n      +- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n         +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n            +- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n               +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n                  +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n                     +- Filter NOT (NOT (anchorage#7066 = null) AND NOT (maneuvering_zone#7068 = null))\n                        +- SubqueryAlias hive_metastore.default.vessel_movements_ppt\n                           +- Relation hive_metastore.default.vessel_movements_ppt[imo#7044L,mmsi#7045L,vessel_id#7046L,vessel_name#7047,date_of_build#7048,vessel_type#7049,group#7050,timestamp#7051,date_only#7052,lon#7053,lat#7054,nav_stat#7055L,speed#7056,course#7057,heading#7058,fuel_category#7059L,main_engine_fuel_type#7060,aux_engine_fuel_type#7061,engine_type#7062,berth#7063,port_name#7064,coverage#7065,anchorage#7066,terminal#7067,... 8 more fields] parquet\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve \"(time_diff <= INTERVAL '10000' SECOND)\" due to data type mismatch: the left and right operands of the binary operator have incompatible types (\"BIGINT\" and \"INTERVAL SECOND\").;\n'Filter ((isnull(time_diff#11777L) = false) OR (time_diff#11777L <= INTERVAL '10000' SECOND))\n+- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n   +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n      +- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n         +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n            +- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n               +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n                  +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n                     +- Filter NOT (NOT (anchorage#7066 = null) AND NOT (maneuvering_zone#7068 = null))\n                        +- SubqueryAlias hive_metastore.default.vessel_movements_ppt\n                           +- Relation hive_metastore.default.vessel_movements_ppt[imo#7044L,mmsi#7045L,vessel_id#7046L,vessel_name#7047,date_of_build#7048,vessel_type#7049,group#7050,timestamp#7051,date_only#7052,lon#7053,lat#7054,nav_stat#7055L,speed#7056,course#7057,heading#7058,fuel_category#7059L,main_engine_fuel_type#7060,aux_engine_fuel_type#7061,engine_type#7062,berth#7063,port_name#7064,coverage#7065,anchorage#7066,terminal#7067,... 8 more fields] parquet\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "window_spec = Window().partitionBy('imo').orderBy('timestamp')\n",
    "step_1 = step_1.withColumn('time_diff', (F.col('timestamp') - F.lag('timestamp').over(window_spec)).cast('long'))\n",
    "\n",
    "# Filter away entries where the consecutive time is more than 10,000 seconds\n",
    "filtered_df = step_1.filter((F.col('time_diff').isNull() == False) | (F.col('time_diff') <= F.expr('INTERVAL 10000 SECONDS')))\n",
    "\n",
    "\n",
    "# Show or use the filtered DataFrame as needed\n",
    "display(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f03ec3ef-6658-48f2-81e1-df31c585ac6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "\u001b[0;32m<command-775644083908624>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n",
       "\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      3\u001b[0m \u001b[0;31m# Assuming 'timeDiff' is the name of your array of structs column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfiltered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time_diff\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time_diff\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"exploded_timeDiff\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      6\u001b[0m \u001b[0;31m# Extract 'seconds' from the exploded struct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n",
       "\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n",
       "\u001b[1;32m   2107\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   2108\u001b[0m         \"\"\"\n",
       "\u001b[0;32m-> 2109\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m   2110\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   2111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n",
       "\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    204\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'explode(time_diff)' due to data type mismatch: input to function explode should be array or map type, not interval day to second;\n",
       "'Project [time_diff#10908, explode(time_diff#10908) AS exploded_timeDiff#11055]\n",
       "+- Filter ((isnull(time_diff#10908) = false) OR (time_diff#10908 <= cast(INTERVAL '10000' SECOND as interval day to second)))\n",
       "   +- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n",
       "      +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n",
       "         +- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n",
       "            +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n",
       "               +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n",
       "                  +- Filter NOT (NOT (anchorage#7066 = null) AND NOT (maneuvering_zone#7068 = null))\n",
       "                     +- SubqueryAlias hive_metastore.default.vessel_movements_ppt\n",
       "                        +- Relation hive_metastore.default.vessel_movements_ppt[imo#7044L,mmsi#7045L,vessel_id#7046L,vessel_name#7047,date_of_build#7048,vessel_type#7049,group#7050,timestamp#7051,date_only#7052,lon#7053,lat#7054,nav_stat#7055L,speed#7056,course#7057,heading#7058,fuel_category#7059L,main_engine_fuel_type#7060,aux_engine_fuel_type#7061,engine_type#7062,berth#7063,port_name#7064,coverage#7065,anchorage#7066,terminal#7067,... 8 more fields] parquet\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n\u001b[0;32m<command-775644083908624>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Assuming 'timeDiff' is the name of your array of structs column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfiltered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time_diff\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time_diff\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"exploded_timeDiff\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Extract 'seconds' from the exploded struct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2107\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2108\u001b[0m         \"\"\"\n\u001b[0;32m-> 2109\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2110\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'explode(time_diff)' due to data type mismatch: input to function explode should be array or map type, not interval day to second;\n'Project [time_diff#10908, explode(time_diff#10908) AS exploded_timeDiff#11055]\n+- Filter ((isnull(time_diff#10908) = false) OR (time_diff#10908 <= cast(INTERVAL '10000' SECOND as interval day to second)))\n   +- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n      +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n         +- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n            +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n               +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n                  +- Filter NOT (NOT (anchorage#7066 = null) AND NOT (maneuvering_zone#7068 = null))\n                     +- SubqueryAlias hive_metastore.default.vessel_movements_ppt\n                        +- Relation hive_metastore.default.vessel_movements_ppt[imo#7044L,mmsi#7045L,vessel_id#7046L,vessel_name#7047,date_of_build#7048,vessel_type#7049,group#7050,timestamp#7051,date_only#7052,lon#7053,lat#7054,nav_stat#7055L,speed#7056,course#7057,heading#7058,fuel_category#7059L,main_engine_fuel_type#7060,aux_engine_fuel_type#7061,engine_type#7062,berth#7063,port_name#7064,coverage#7065,anchorage#7066,terminal#7067,... 8 more fields] parquet\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: cannot resolve 'explode(time_diff)' due to data type mismatch: input to function explode should be array or map type, not interval day to second;\n'Project [time_diff#10908, explode(time_diff#10908) AS exploded_timeDiff#11055]\n+- Filter ((isnull(time_diff#10908) = false) OR (time_diff#10908 <= cast(INTERVAL '10000' SECOND as interval day to second)))\n   +- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n      +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n         +- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n            +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n               +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n                  +- Filter NOT (NOT (anchorage#7066 = null) AND NOT (maneuvering_zone#7068 = null))\n                     +- SubqueryAlias hive_metastore.default.vessel_movements_ppt\n                        +- Relation hive_metastore.default.vessel_movements_ppt[imo#7044L,mmsi#7045L,vessel_id#7046L,vessel_name#7047,date_of_build#7048,vessel_type#7049,group#7050,timestamp#7051,date_only#7052,lon#7053,lat#7054,nav_stat#7055L,speed#7056,course#7057,heading#7058,fuel_category#7059L,main_engine_fuel_type#7060,aux_engine_fuel_type#7061,engine_type#7062,berth#7063,port_name#7064,coverage#7065,anchorage#7066,terminal#7067,... 8 more fields] parquet\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from pyspark.sql.functions import explode, col\n",
    "\n",
    "# # Assuming 'timeDiff' is the name of your array of structs column\n",
    "# filtered_df = filtered_df.select(\"time_diff\", explode(\"time_diff\").alias(\"exploded_timeDiff\"))\n",
    "\n",
    "# # Extract 'seconds' from the exploded struct\n",
    "# filtered_df = filtered_df.withColumn(\"seconds_column\", col(\"exploded_timeDiff.seconds\"))\n",
    "\n",
    "# # Drop the exploded column if needed\n",
    "# filtered_df = filtered_df.drop(\"exploded_timeDiff\")\n",
    "\n",
    "# # Show the updated DataFrame\n",
    "# display(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "722eb503-a63a-4856-82e8-09be19ea60b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
       "\u001b[0;32m<command-775644083908619>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_diff'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\n",
       "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n\u001b[0;32m<command-775644083908619>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_diff'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: 'Column' object is not callable",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_df['time_diff'].seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcb10247-4c14-4654-b649-4ed171e7acaa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "\u001b[0;32m<command-775644083908625>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n",
       "\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfiltered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'seconds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time_diff.seconds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n",
       "\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n",
       "\u001b[1;32m   3325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   3326\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m-> 3327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m   3328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   3329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n",
       "\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    204\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: Can't extract value from time_diff#10908: need struct type but got interval day to second; line 1 pos 0"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n\u001b[0;32m<command-775644083908625>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfiltered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'seconds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time_diff.seconds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   3325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3326\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mAnalysisException\u001b[0m: Can't extract value from time_diff#10908: need struct type but got interval day to second; line 1 pos 0",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Can't extract value from time_diff#10908: need struct type but got interval day to second; line 1 pos 0",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "filtered_df = filtered_df.withColumn('seconds', expr('time_diff.seconds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c1a13dc-20ef-4205-b880-ab4953d9de9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "\u001b[0;32m<command-775644083908626>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n",
       "\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      3\u001b[0m \u001b[0;31m# Extract the \"seconds\" field from the \"timeDiff\" column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfiltered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"seconds_column\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time_diff.seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      6\u001b[0m \u001b[0;31m# Show the updated DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n",
       "\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n",
       "\u001b[1;32m   3325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   3326\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m-> 3327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m   3328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   3329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n",
       "\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    204\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: Can't extract value from time_diff#10908: need struct type but got interval day to second"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n\u001b[0;32m<command-775644083908626>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Extract the \"seconds\" field from the \"timeDiff\" column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfiltered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"seconds_column\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time_diff.seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Show the updated DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   3325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3326\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mAnalysisException\u001b[0m: Can't extract value from time_diff#10908: need struct type but got interval day to second",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Can't extract value from time_diff#10908: need struct type but got interval day to second",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Extract the \"seconds\" field from the \"timeDiff\" column\n",
    "filtered_df = filtered_df.withColumn(\"seconds_column\", col(\"time_diff.seconds\"))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f77c316-f7ae-4724-bff1-4b7b1893bdde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "\u001b[0;32m<command-775644083908631>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n",
       "\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoubleType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      3\u001b[0m \u001b[0;31m# Convert `time_diff` to string and extract seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnewDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time_diff\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_diff\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoubleType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      6\u001b[0m \u001b[0;31m# Show the updated DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n",
       "\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n",
       "\u001b[1;32m   3325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   3326\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m-> 3327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m   3328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   3329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n",
       "\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    204\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'CAST(time_diff AS DOUBLE)' due to data type mismatch: cannot cast interval day to second to double;\n",
       "'Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 11 more fields]\n",
       "+- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 11 more fields]\n",
       "   +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 11 more fields]\n",
       "      +- Filter ((isnull(time_diff#10908) = false) OR (time_diff#10908 <= cast(INTERVAL '10000' SECOND as interval day to second)))\n",
       "         +- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n",
       "            +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n",
       "               +- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n",
       "                  +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n",
       "                     +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n",
       "                        +- Filter NOT (NOT (anchorage#7066 = null) AND NOT (maneuvering_zone#7068 = null))\n",
       "                           +- SubqueryAlias hive_metastore.default.vessel_movements_ppt\n",
       "                              +- Relation hive_metastore.default.vessel_movements_ppt[imo#7044L,mmsi#7045L,vessel_id#7046L,vessel_name#7047,date_of_build#7048,vessel_type#7049,group#7050,timestamp#7051,date_only#7052,lon#7053,lat#7054,nav_stat#7055L,speed#7056,course#7057,heading#7058,fuel_category#7059L,main_engine_fuel_type#7060,aux_engine_fuel_type#7061,engine_type#7062,berth#7063,port_name#7064,coverage#7065,anchorage#7066,terminal#7067,... 8 more fields] parquet\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n\u001b[0;32m<command-775644083908631>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoubleType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Convert `time_diff` to string and extract seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnewDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time_diff\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_diff\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoubleType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Show the updated DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   3325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3326\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'CAST(time_diff AS DOUBLE)' due to data type mismatch: cannot cast interval day to second to double;\n'Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 11 more fields]\n+- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 11 more fields]\n   +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 11 more fields]\n      +- Filter ((isnull(time_diff#10908) = false) OR (time_diff#10908 <= cast(INTERVAL '10000' SECOND as interval day to second)))\n         +- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n            +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n               +- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n                  +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n                     +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n                        +- Filter NOT (NOT (anchorage#7066 = null) AND NOT (maneuvering_zone#7068 = null))\n                           +- SubqueryAlias hive_metastore.default.vessel_movements_ppt\n                              +- Relation hive_metastore.default.vessel_movements_ppt[imo#7044L,mmsi#7045L,vessel_id#7046L,vessel_name#7047,date_of_build#7048,vessel_type#7049,group#7050,timestamp#7051,date_only#7052,lon#7053,lat#7054,nav_stat#7055L,speed#7056,course#7057,heading#7058,fuel_category#7059L,main_engine_fuel_type#7060,aux_engine_fuel_type#7061,engine_type#7062,berth#7063,port_name#7064,coverage#7065,anchorage#7066,terminal#7067,... 8 more fields] parquet\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: cannot resolve 'CAST(time_diff AS DOUBLE)' due to data type mismatch: cannot cast interval day to second to double;\n'Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 11 more fields]\n+- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 11 more fields]\n   +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 11 more fields]\n      +- Filter ((isnull(time_diff#10908) = false) OR (time_diff#10908 <= cast(INTERVAL '10000' SECOND as interval day to second)))\n         +- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n            +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n               +- Window [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 10 more fields], [imo#7044L], [timestamp#7051 ASC NULLS FIRST]\n                  +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n                     +- Project [imo#7044L, mmsi#7045L, vessel_id#7046L, vessel_name#7047, date_of_build#7048, vessel_type#7049, group#7050, timestamp#7051, date_only#7052, lon#7053, lat#7054, nav_stat#7055L, speed#7056, course#7057, heading#7058, fuel_category#7059L, main_engine_fuel_type#7060, aux_engine_fuel_type#7061, engine_type#7062, berth#7063, port_name#7064, coverage#7065, anchorage#7066, terminal#7067, ... 9 more fields]\n                        +- Filter NOT (NOT (anchorage#7066 = null) AND NOT (maneuvering_zone#7068 = null))\n                           +- SubqueryAlias hive_metastore.default.vessel_movements_ppt\n                              +- Relation hive_metastore.default.vessel_movements_ppt[imo#7044L,mmsi#7045L,vessel_id#7046L,vessel_name#7047,date_of_build#7048,vessel_type#7049,group#7050,timestamp#7051,date_only#7052,lon#7053,lat#7054,nav_stat#7055L,speed#7056,course#7057,heading#7058,fuel_category#7059L,main_engine_fuel_type#7060,aux_engine_fuel_type#7061,engine_type#7062,berth#7063,port_name#7064,coverage#7065,anchorage#7066,terminal#7067,... 8 more fields] parquet\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.types import DoubleType\n",
    "# Convert `time_diff` to string and extract seconds\n",
    "newDf = filtered_df.withColumn(\"time_diff\", filtered_df[\"time_diff\"].cast(DoubleType()))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "newDf.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "886a2242-df23-405e-83e0-5535a049ae8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/conversion.py:208: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.pyspark.fallback.enabled' does not have an effect on failures in the middle of computation.\n",
      "  An error occurred while calling o3386.getResult.\n",
      ": org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:454)\n",
      "\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n",
      "\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 111.0 failed 4 times, most recent failure: Lost task 4.3 in stage 111.0 (TID 133) (10.139.64.4 executor 0): java.lang.IndexOutOfBoundsException: index: 504, length: 1 (expected: range(0, 504))\n",
      "\tat org.apache.arrow.memory.ArrowBuf.checkIndexD(ArrowBuf.java:318)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.chk(ArrowBuf.java:305)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.getByte(ArrowBuf.java:507)\n",
      "\tat org.apache.arrow.vector.BitVectorHelper.setBit(BitVectorHelper.java:85)\n",
      "\tat org.apache.arrow.vector.DurationVector.set(DurationVector.java:229)\n",
      "\tat org.apache.spark.sql.execution.arrow.DurationWriter.setValue(ArrowWriter.scala:451)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowFieldWriter.write(ArrowWriter.scala:141)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowWriter.write(ArrowWriter.scala:95)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:172)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:181)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:145)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:4232)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2840)\n",
      "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
      "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
      "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
      "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)\n",
      "\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:137)\n",
      "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n",
      "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:137)\n",
      "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3381)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3313)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3304)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3304)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1428)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1428)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1428)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3593)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3531)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3519)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1177)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1165)\n",
      "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2746)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2729)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2841)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:4230)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:4234)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:4200)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4297)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:773)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4295)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:399)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:194)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:148)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:349)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4295)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:4200)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:4199)\n",
      "\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n",
      "\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n",
      "\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n",
      "\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n",
      "\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\n",
      "Caused by: java.lang.IndexOutOfBoundsException: index: 504, length: 1 (expected: range(0, 504))\n",
      "\tat org.apache.arrow.memory.ArrowBuf.checkIndexD(ArrowBuf.java:318)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.chk(ArrowBuf.java:305)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.getByte(ArrowBuf.java:507)\n",
      "\tat org.apache.arrow.vector.BitVectorHelper.setBit(BitVectorHelper.java:85)\n",
      "\tat org.apache.arrow.vector.DurationVector.set(DurationVector.java:229)\n",
      "\tat org.apache.spark.sql.execution.arrow.DurationWriter.setValue(ArrowWriter.scala:451)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowFieldWriter.write(ArrowWriter.scala:141)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowWriter.write(ArrowWriter.scala:95)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:172)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:181)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:145)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:4232)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2840)\n",
      "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
      "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
      "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
      "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)\n",
      "\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:137)\n",
      "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n",
      "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:137)\n",
      "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n",
       "\u001b[0;32m<command-775644083908636>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpandas_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpandas_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n",
       "\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/databricks/utils/instrumentation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     41\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m     42\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mreturn_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m     45\u001b[0m             \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n",
       "\u001b[1;32m    145\u001b[0m                     \u001b[0mtmp_column_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"col_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    146\u001b[0m                     \u001b[0mself_destruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrowPySparkSelfDestructEnabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m--> 147\u001b[0;31m                     batches = self.toDF(*tmp_column_names)._collect_as_arrow(\n",
       "\u001b[0m\u001b[1;32m    148\u001b[0m                         \u001b[0msplit_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_destruct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    149\u001b[0m                     )\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36m_collect_as_arrow\u001b[0;34m(self, split_batches)\u001b[0m\n",
       "\u001b[1;32m    365\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    366\u001b[0m             \u001b[0;31m# Join serving thread and raise any exceptions from collectAsArrowToPython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m--> 367\u001b[0;31m             \u001b[0mjsocket_auth_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    369\u001b[0m         \u001b[0;31m# Separate RecordBatches from batch order indices in results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n",
       "\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m--> 196\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    198\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
       "\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n",
       "\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
       "\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3386.getResult.\n",
       ": org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
       "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:454)\n",
       "\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n",
       "\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 111.0 failed 4 times, most recent failure: Lost task 4.3 in stage 111.0 (TID 133) (10.139.64.4 executor 0): java.lang.IndexOutOfBoundsException: index: 504, length: 1 (expected: range(0, 504))\n",
       "\tat org.apache.arrow.memory.ArrowBuf.checkIndexD(ArrowBuf.java:318)\n",
       "\tat org.apache.arrow.memory.ArrowBuf.chk(ArrowBuf.java:305)\n",
       "\tat org.apache.arrow.memory.ArrowBuf.getByte(ArrowBuf.java:507)\n",
       "\tat org.apache.arrow.vector.BitVectorHelper.setBit(BitVectorHelper.java:85)\n",
       "\tat org.apache.arrow.vector.DurationVector.set(DurationVector.java:229)\n",
       "\tat org.apache.spark.sql.execution.arrow.DurationWriter.setValue(ArrowWriter.scala:451)\n",
       "\tat org.apache.spark.sql.execution.arrow.ArrowFieldWriter.write(ArrowWriter.scala:141)\n",
       "\tat org.apache.spark.sql.execution.arrow.ArrowWriter.write(ArrowWriter.scala:95)\n",
       "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:172)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n",
       "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:181)\n",
       "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:145)\n",
       "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
       "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
       "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
       "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
       "\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n",
       "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
       "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
       "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n",
       "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
       "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
       "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:4232)\n",
       "\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2840)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:137)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:137)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3381)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3313)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3304)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3304)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1428)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1428)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1428)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3593)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3531)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3519)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1177)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1165)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2746)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2729)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2841)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:4230)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:4234)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:4200)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4297)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:773)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4295)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:249)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:399)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:194)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:148)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:349)\n",
       "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4295)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:4200)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:4199)\n",
       "\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n",
       "\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n",
       "\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n",
       "\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n",
       "\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n",
       "\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\n",
       "Caused by: java.lang.IndexOutOfBoundsException: index: 504, length: 1 (expected: range(0, 504))\n",
       "\tat org.apache.arrow.memory.ArrowBuf.checkIndexD(ArrowBuf.java:318)\n",
       "\tat org.apache.arrow.memory.ArrowBuf.chk(ArrowBuf.java:305)\n",
       "\tat org.apache.arrow.memory.ArrowBuf.getByte(ArrowBuf.java:507)\n",
       "\tat org.apache.arrow.vector.BitVectorHelper.setBit(BitVectorHelper.java:85)\n",
       "\tat org.apache.arrow.vector.DurationVector.set(DurationVector.java:229)\n",
       "\tat org.apache.spark.sql.execution.arrow.DurationWriter.setValue(ArrowWriter.scala:451)\n",
       "\tat org.apache.spark.sql.execution.arrow.ArrowFieldWriter.write(ArrowWriter.scala:141)\n",
       "\tat org.apache.spark.sql.execution.arrow.ArrowWriter.write(ArrowWriter.scala:95)\n",
       "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:172)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n",
       "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:181)\n",
       "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:145)\n",
       "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
       "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
       "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
       "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
       "\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n",
       "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
       "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
       "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n",
       "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
       "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
       "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:4232)\n",
       "\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2840)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:137)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:137)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n\u001b[0;32m<command-775644083908636>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpandas_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpandas_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/databricks/utils/instrumentation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mreturn_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0mtmp_column_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"col_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                     \u001b[0mself_destruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrowPySparkSelfDestructEnabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                     batches = self.toDF(*tmp_column_names)._collect_as_arrow(\n\u001b[0m\u001b[1;32m    148\u001b[0m                         \u001b[0msplit_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_destruct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                     )\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36m_collect_as_arrow\u001b[0;34m(self, split_batches)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;31m# Join serving thread and raise any exceptions from collectAsArrowToPython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0mjsocket_auth_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;31m# Separate RecordBatches from batch order indices in results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n\n\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3386.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:454)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 111.0 failed 4 times, most recent failure: Lost task 4.3 in stage 111.0 (TID 133) (10.139.64.4 executor 0): java.lang.IndexOutOfBoundsException: index: 504, length: 1 (expected: range(0, 504))\n\tat org.apache.arrow.memory.ArrowBuf.checkIndexD(ArrowBuf.java:318)\n\tat org.apache.arrow.memory.ArrowBuf.chk(ArrowBuf.java:305)\n\tat org.apache.arrow.memory.ArrowBuf.getByte(ArrowBuf.java:507)\n\tat org.apache.arrow.vector.BitVectorHelper.setBit(BitVectorHelper.java:85)\n\tat org.apache.arrow.vector.DurationVector.set(DurationVector.java:229)\n\tat org.apache.spark.sql.execution.arrow.DurationWriter.setValue(ArrowWriter.scala:451)\n\tat org.apache.spark.sql.execution.arrow.ArrowFieldWriter.write(ArrowWriter.scala:141)\n\tat org.apache.spark.sql.execution.arrow.ArrowWriter.write(ArrowWriter.scala:95)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:172)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:181)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:145)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:4232)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2840)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:137)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:137)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3381)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3313)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3304)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3304)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1428)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1428)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1428)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3593)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3531)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3519)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1177)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1165)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2746)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2729)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2841)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:4230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:4234)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:4200)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4297)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:773)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4295)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:249)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:399)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:194)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:148)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:349)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4295)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:4200)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:4199)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\nCaused by: java.lang.IndexOutOfBoundsException: index: 504, length: 1 (expected: range(0, 504))\n\tat org.apache.arrow.memory.ArrowBuf.checkIndexD(ArrowBuf.java:318)\n\tat org.apache.arrow.memory.ArrowBuf.chk(ArrowBuf.java:305)\n\tat org.apache.arrow.memory.ArrowBuf.getByte(ArrowBuf.java:507)\n\tat org.apache.arrow.vector.BitVectorHelper.setBit(BitVectorHelper.java:85)\n\tat org.apache.arrow.vector.DurationVector.set(DurationVector.java:229)\n\tat org.apache.spark.sql.execution.arrow.DurationWriter.setValue(ArrowWriter.scala:451)\n\tat org.apache.spark.sql.execution.arrow.ArrowFieldWriter.write(ArrowWriter.scala:141)\n\tat org.apache.spark.sql.execution.arrow.ArrowWriter.write(ArrowWriter.scala:95)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:172)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:181)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:145)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:4232)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2840)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:137)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:137)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "org.apache.spark.SparkException: Exception thrown in awaitResult: ",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dc08bb6-83f3-478c-8ec5-27468bb47e03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "EDA",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
